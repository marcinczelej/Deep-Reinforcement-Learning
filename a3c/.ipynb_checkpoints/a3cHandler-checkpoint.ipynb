{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import a3cNetwork\n",
    "import numpy as np\n",
    "from sharedOptimizer import sharedAdam\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from test import test\n",
    "from train import train\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class params:\n",
    "    seed = 4\n",
    "    env_name = \"CartPole-v0\"\n",
    "    state_space = 4\n",
    "    action_space = 2\n",
    "    n_steps = 10\n",
    "    gamma = 0.99\n",
    "    tau = 0.5\n",
    "    lr = 0.001\n",
    "    number_of_processes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10  reward =  200.0 , | means score =  74.1  | max reward =  200.0\n",
      "20  reward =  65.0 , | means score =  102.75  | max reward =  200.0\n",
      "30  reward =  72.0 , | means score =  108.13333333333334  | max reward =  200.0\n",
      "40  reward =  200.0 , | means score =  120.3  | max reward =  200.0\n",
      "50  reward =  125.0 , | means score =  133.7  | max reward =  200.0\n",
      "60  reward =  200.0 , | means score =  144.75  | max reward =  200.0\n",
      "70  reward =  200.0 , | means score =  152.64285714285714  | max reward =  200.0\n",
      "80  reward =  200.0 , | means score =  157.525  | max reward =  200.0\n",
      "90  reward =  73.0 , | means score =  160.83333333333334  | max reward =  200.0\n",
      "100  reward =  200.0 , | means score =  159.73  | max reward =  200.0\n"
     ]
    }
   ],
   "source": [
    "shared_model = a3cNetwork(params.state_space, params.action_space, device)\n",
    "shared_model.share_memory()\n",
    "\n",
    "shared_optimizer = sharedAdam(shared_model.parameters(), lr = params.lr)\n",
    "\n",
    "processes = []\n",
    "p = mp.Process(target = test, args = (params, shared_model, device))\n",
    "p.start()\n",
    "processes.append(p)\n",
    "\n",
    "for i in range(params.number_of_processes):\n",
    "    p = mp.Process(target = train, args = (i, params, shared_model, shared_optimizer, device))\n",
    "    p.start()\n",
    "    processes.append(p)\n",
    "for i in range(len(processes)):\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deep_reinforcement)",
   "language": "python",
   "name": "deep_reinforcement"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
