{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Policy, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(state_size, 32)\n",
    "        self.layer_2 = nn.Linear(32, 16)\n",
    "        self.critic_layer = nn.Linear(32, 1)\n",
    "        self.actor_layer = nn.Linear(32, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        if state.ndim == 1:\n",
    "            input =  torch.from_numpy(state).float().unsqueeze(0).to(\"cpu\")\n",
    "        else:\n",
    "            input =  torch.from_numpy(state).float().to(\"cpu\")\n",
    "        \n",
    "        x = F.relu(self.layer_1(input))\n",
    "        \n",
    "        return self.actor_head(x), self.critic_layer(x)\n",
    "    \n",
    "    def actor_head(self, input_state):\n",
    "        x = self.actor_layer(input_state)\n",
    "        prob = F.softmax(x, dim=1)\n",
    "        log_prob = F.log_softmax(x, dim=1)\n",
    "        action = prob.multinomial(1)\n",
    "        entropy = (-prob*log_prob).sum(-1).mean()\n",
    "        return {\"action\": action, \"log_prob\":log_prob.gather(1, action), \"entropy\":entropy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "old_policy = Policy(4, 2)\n",
    "new_policy = Policy(4, 2)\n",
    "optimizer = optim.Adam(new_policy.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_trajectory(policy, env, max_steps):\n",
    "    rewards = []\n",
    "    values = []\n",
    "    states = []\n",
    "    dones = []\n",
    "    actions = []\n",
    "    entropys = []\n",
    "    log_probs = []\n",
    "\n",
    "    i = 0\n",
    "    done = True\n",
    "    state = env.reset()\n",
    "    reward_sum = 0\n",
    "    \n",
    "    while True:\n",
    "        actor, value = policy(state)\n",
    "        \n",
    "        if i > 0 and i >= max_steps:\n",
    "            yield {\"states\": states, \"actions\": actions, \"rewards\": rewards, \"log_probs\": log_probs,\n",
    "                  \"last_value\": value*(1-done), \"dones\":dones, \"values\":values}\n",
    "\n",
    "        actions.append(actor[\"action\"])\n",
    "        states.append(state)\n",
    "        values.append(value)\n",
    "        action = actor[\"action\"].numpy()[0][0]\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        log_probs.append(actor[\"log_prob\"])\n",
    "        \n",
    "        dones.append(done)\n",
    "        rewards.append(reward)\n",
    "        reward_sum += reward\n",
    "        i+=1\n",
    "        if done:\n",
    "            print(\" episode reward : \", reward_sum)\n",
    "            reward_sum = 0\n",
    "            state = env.reset()\n",
    "\n",
    "def calculate_gae(obs, gamma, tau):\n",
    "    values = obs[\"values\"]\n",
    "    values.append(obs[\"last_value\"])\n",
    "    rewards = obs[\"rewards\"]\n",
    "    dones = obs[\"dones\"]\n",
    "    gae = torch.zeros(len(rewards))\n",
    "    refs = []\n",
    "    \n",
    "    last_gae = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        td_error = rewards[i] + gamma*values[i+1]*(1-dones[i]) - values[i]\n",
    "        gae[i] = gamma*tau*last_gae + td_error\n",
    "        last_gae = gae[i]\n",
    "        refs.append(last_gae - values[i])\n",
    "    obs[\"advantages\"] = torch.FloatTensor(gae).to(device)\n",
    "    obs[\"ref\"] = torch.FloatTensor(refs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " episode reward :  14.0\n",
      " episode reward :  25.0\n",
      " episode reward :  13.0\n",
      " episode reward :  25.0\n",
      " episode reward :  19.0\n",
      " episode reward :  35.0\n",
      " episode reward :  23.0\n",
      " episode reward :  31.0\n",
      " episode reward :  17.0\n",
      " episode reward :  23.0\n",
      " episode reward :  17.0\n",
      " episode reward :  14.0\n",
      " episode reward :  39.0\n",
      " episode reward :  18.0\n",
      " episode reward :  11.0\n",
      " episode reward :  17.0\n",
      " episode reward :  25.0\n",
      " episode reward :  19.0\n",
      " episode reward :  12.0\n",
      " episode reward :  12.0\n",
      " episode reward :  15.0\n",
      " episode reward :  23.0\n",
      " episode reward :  13.0\n",
      " episode reward :  16.0\n",
      " episode reward :  18.0\n",
      " episode reward :  14.0\n",
      " episode reward :  16.0\n",
      " episode reward :  14.0\n",
      " episode reward :  13.0\n",
      " episode reward :  14.0\n",
      " episode reward :  19.0\n",
      " episode reward :  13.0\n",
      " episode reward :  13.0\n",
      " episode reward :  14.0\n",
      " episode reward :  18.0\n",
      " episode reward :  11.0\n",
      " episode reward :  23.0\n",
      " episode reward :  27.0\n",
      " episode reward :  12.0\n",
      " episode reward :  18.0\n",
      " episode reward :  17.0\n",
      " episode reward :  32.0\n",
      " episode reward :  21.0\n",
      " episode reward :  11.0\n",
      " episode reward :  14.0\n",
      " episode reward :  19.0\n",
      " episode reward :  14.0\n",
      " episode reward :  23.0\n",
      " episode reward :  16.0\n",
      " episode reward :  19.0\n",
      " episode reward :  12.0\n",
      " episode reward :  9.0\n",
      " episode reward :  9.0\n",
      " episode reward :  11.0\n",
      " episode reward :  17.0\n",
      " episode reward :  10.0\n"
     ]
    }
   ],
   "source": [
    "data = collect_trajectory(old_policy, env, 1000)\n",
    "observations = data.__next__()\n",
    "calculate_gae(observations, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def surrogate_method(new_policy, obs, epsilon = 0.1, beta = 0.01):\n",
    "    \n",
    "    actions = torch.tensor(obs[\"actions\"], dtype=torch.int8, device=device)\n",
    "    states = torch.FloatTensor(obs[\"states\"]).to(device)\n",
    "    old_log_probs = torch.FloatTensor(obs[\"log_probs\"]).to(device)\n",
    "    \n",
    "    actor, value = new_policy(states.cpu().numpy())\n",
    "    \n",
    "    new_log_probs = torch.FloatTensor(actor[\"log_prob\"].squeeze(0)).to(device)\n",
    "    action = actor[\"action\"].to(device)\n",
    "    \n",
    "    ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "    \n",
    "    normalized_adv = (obs[\"advantages\"] - obs[\"advantages\"].mean())/ (obs[\"advantages\"].std() + 1e-5)\n",
    "    \n",
    "    cliped_ratio = torch.clamp(ratio, 1-epsilon, 1+epsilon)*normalized_adv\n",
    "    no_clipped_ratio = ratio*normalized_adv\n",
    "    action_loss = -torch.min(no_clipped_ratio, cliped_ratio).mean()\n",
    "    policy_loss = (obs[\"ref\"] - value.to(device)).pow(2).mean()\n",
    "    \n",
    "    return torch.mean(ratio + policy_loss - beta*actor[\"entropy\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e+05 *\n",
      "       3.2943, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "Loss = surrogate_method(new_policy, observations)\n",
    "print(Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/dai-dao/PPO-Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
