{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from MADDPG_wrapper import MADDPGWrapper\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class params():\n",
    "    buffer_size = 100000\n",
    "    batch_size = 128\n",
    "    tau = 0.008\n",
    "    actor_lr = 0.0001\n",
    "    critic_lr = 0.001\n",
    "    actor_weight_decay = 0\n",
    "    critic_weight_decay = 0\n",
    "    gamma = 0.99\n",
    "    updates_per_step = 3\n",
    "    atoms_number = 51\n",
    "    V_min = -5\n",
    "    V_max = 5\n",
    "    delta = (V_max - V_min)/(atoms_number -1)\n",
    "    n_steps = 5\n",
    "    EPS_START = 5.0         # initial value for epsilon in noise decay process in Agent.act()\n",
    "    EPS_EP_END = 300        # episode to end the noise decay process\n",
    "    EPS_FINAL = 0 # final value for epsilon after decay\n",
    "\n",
    "MADDPG_agent = MADDPGWrapper(action_size, state_size, device, params, num_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepopulation start\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "92000\n",
      "93000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "prepopulation end\n"
     ]
    }
   ],
   "source": [
    "MADDPG_agent.prepopulateBuffer(brain_name, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 : Total score (averaged over agents): 0.017701896263778806  reward this episode:  0.0 max:  0.296029904411\n",
      "110 : Total score (averaged over agents): 0.015741696234569547  reward this episode:  0.0 max:  0.296029904411\n",
      "120 : Total score (averaged over agents): 0.00886089813203767  reward this episode:  0.0 max:  0.296029904411\n",
      "130 : Total score (averaged over agents): 0.012781596190460622  reward this episode:  0.194059802892 max:  0.298010004441\n",
      "140 : Total score (averaged over agents): 0.011791596175708472  reward this episode:  0.0 max:  0.298010004441\n",
      "150 : Total score (averaged over agents): 0.03943399058761224  reward this episode:  0.587099708748 max:  0.888099713234\n",
      "160 : Total score (averaged over agents): 0.052235087778363457  reward this episode:  0.0 max:  0.888099713234\n",
      "170 : Total score (averaged over agents): 0.07293698308684574  reward this episode:  0.492059807332 max:  0.888099713234\n",
      "180 : Total score (averaged over agents): 0.13610227002808187  reward this episode:  0.0 max:  2.16723933229\n",
      "190 : Total score (averaged over agents): 0.21103895514472545  reward this episode:  0.196029902921 max:  3.94435925878\n",
      "200 : Total score (averaged over agents): 0.22282075032028786  reward this episode:  0.0 max:  3.94435925878\n",
      "210 : Total score (averaged over agents): 0.2307114484378684  reward this episode:  0.0990000014752 max:  3.94435925878\n",
      "220 : Total score (averaged over agents): 0.2287214484082151  reward this episode:  0.0 max:  3.94435925878\n",
      "230 : Total score (averaged over agents): 0.22282065032028636  reward this episode:  0.0 max:  3.94435925878\n",
      "240 : Total score (averaged over agents): 0.22970144842281823  reward this episode:  0.0 max:  3.94435925878\n",
      "250 : Total score (averaged over agents): 0.2040292530402727  reward this episode:  0.0 max:  3.94435925878\n",
      "260 : Total score (averaged over agents): 0.1991684549678412  reward this episode:  0.0 max:  3.94435925878\n",
      "270 : Total score (averaged over agents): 0.20411905304161085  reward this episode:  0.0 max:  3.94435925878\n",
      "280 : Total score (averaged over agents): 0.1448843641589452  reward this episode:  0.0 max:  3.94435925878\n",
      "290 : Total score (averaged over agents): 0.0738978791011642  reward this episode:  0.0 max:  3.94435925878\n",
      "300 : Total score (averaged over agents): 0.0679672810127914  reward this episode:  0.0970299014459 max:  3.94435925878\n",
      "310 : Total score (averaged over agents): 0.17949606367469972  reward this episode:  0.397000005916 max:  3.95723975897\n",
      "320 : Total score (averaged over agents): 0.27796653614202405  reward this episode:  0.78312961167 max:  3.95723975897\n",
      "330 : Total score (averaged over agents): 0.4030389070057477  reward this episode:  1.67516962496 max:  3.95723975897\n",
      "340 : Total score (averaged over agents): 0.47894619013685436  reward this episode:  0.196029902921 max:  4.73542907056\n",
      "Reached   0.543873473104  in episode  341\n",
      "350 : Total score (averaged over agents): 1.1281914368113617  reward this episode:  3.83938915721 max:  17.047676154\n",
      "360 : Total score (averaged over agents): 1.3400128879677473  reward this episode:  0.297000004426 max:  17.047676154\n",
      "370 : Total score (averaged over agents): 1.8011679868394936  reward this episode:  1.77514972645 max:  17.047676154\n",
      "380 : Total score (averaged over agents): 2.4218309340880912  reward this episode:  0.88113961313 max:  24.7383444686\n",
      "390 : Total score (averaged over agents): 2.680926473948915  reward this episode:  0.693059810327 max:  24.7383444686\n",
      "400 : Total score (averaged over agents): 3.1521895789712824  reward this episode:  3.7443691558 max:  24.7383444686\n",
      "410 : Total score (averaged over agents): 3.6032254657145923  reward this episode:  2.0672294308 max:  24.7383444686\n",
      "420 : Total score (averaged over agents): 4.432868471077236  reward this episode:  18.7168854789 max:  24.7383444686\n",
      "430 : Total score (averaged over agents): 5.243540777160917  reward this episode:  14.4496660157 max:  24.7383444686\n",
      "440 : Total score (averaged over agents): 5.430758137950675  reward this episode:  0.196029902921 max:  24.7383444686\n"
     ]
    }
   ],
   "source": [
    "episode_nr = 0\n",
    "finish_score = 0.5\n",
    "last_100_scores = deque(maxlen=100)\n",
    "scores_all = []\n",
    "max_reward = 0\n",
    "finished = False\n",
    "finish_episode_nr = 0\n",
    "episodes_after = 100\n",
    "\n",
    "while True:\n",
    "    if episode_nr%100 == 0:\n",
    "        name = str(episode_nr) + '.pth'\n",
    "        MADDPG_agent.save_model(name)\n",
    "    episode_nr += 1\n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    next_states = states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        n_step_rewards = np.zeros(num_agents)\n",
    "        for i in range(params.n_steps-1):\n",
    "            states = next_states \n",
    "            actions = MADDPG_agent.select_actions(states)\n",
    "            env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "            next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "            rewards = env_info.rewards                         # get reward (for each agent)\n",
    "            n_step_rewards += [reward * (params.gamma**i) for reward in rewards]\n",
    "            dones = env_info.local_done                        # see if episode finished\n",
    "        scores += np.max(n_step_rewards)                         # update the score (for each agent)\n",
    "        MADDPG_agent.step(states, actions, n_step_rewards, next_states, dones)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    last_100_scores.append(np.max(scores))\n",
    "    scores_all.append(np.max(scores))\n",
    "    if np.max(scores) > max_reward:\n",
    "        max_reward = np.max(scores)\n",
    "    if episode_nr >= 100 and np.mean(last_100_scores) >= finish_score and finished == False:\n",
    "        print(\"Reached  \", np.mean(last_100_scores), \" in episode \", episode_nr)\n",
    "        MADDPG_agent.save_model('final.pth')\n",
    "        finished = True\n",
    "        finish_episode_nr = episode_nr\n",
    "        \n",
    "    if episode_nr%10 ==0 and episode_nr >= 100:\n",
    "        print(episode_nr, ': Total score (averaged over agents): {}'.format(np.mean(last_100_scores)), \" reward this episode: \", np.max(scores), \"max: \", max_reward)\n",
    "    \n",
    "    if finished == True and episode_nr >= (finish_episode_nr + episodes_after):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f783eeed080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_all)), scores_all, label='MADDPG')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='upper left');\n",
    "plt.show()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "fig.savefig('result.png', dpi=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  0\n",
      "episode  0  score :  0.10000000149\n",
      "episode  1\n",
      "episode  1  score :  1.10000001639\n",
      "episode  2\n",
      "episode  2  score :  1.50000002235\n",
      "episode  3\n",
      "episode  3  score :  5.20000007749\n",
      "episode  4\n",
      "episode  4  score :  0.20000000298\n"
     ]
    }
   ],
   "source": [
    "MADDPG_agent.load_model()\n",
    "episode = 0\n",
    "scores =0\n",
    "for episode in range(5):\n",
    "    print(\"episode \", episode)\n",
    "    scores =0\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    next_states = states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    while True:\n",
    "        actions = MADDPG_agent.select_actions(states, False)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += np.max(rewards)                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print(\"episode \", episode, \" score : \", np.max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
