{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"C:/Users/Marcin i Ewa/Udacity/deep-reinforcement-learning/p1_navigation/Banana_Windows_x86_64/Banana.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Number of actions: 4\n",
      "<unityagents.brain.BrainInfo object at 0x000001DB5D764198>\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "print(env_info)\n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action (uniformly) at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "import torch\n",
    "episode_number = 1500\n",
    "eps_start=1.0\n",
    "eps_end=0.01\n",
    "eps_decay=0.995\n",
    "max_steps = 500\n",
    "agent = Agent(action_number = brain.vector_action_space_size, state_size = len(env_info.vector_observations[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    print(\"starting preprocessing\")\n",
    "    end_preprocessing = False\n",
    "    while not end_preprocessing:\n",
    "        print(len(agent.memoryBuffer))\n",
    "        env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations[0]            # get the current state\n",
    "        while True:\n",
    "            action = np.random.choice(brain.vector_action_space_size)            # select an action\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.memoryBuffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state                             # roll over the state to next time step\n",
    "            if len(agent.memoryBuffer) == 4999:\n",
    "                end_preprocessing = True\n",
    "            if done or len(agent.memoryBuffer) >=4999:\n",
    "                break\n",
    "    print(\"preprocessing done\", len(agent.memoryBuffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting preprocessing\n",
      "0\n",
      "300\n",
      "600\n",
      "900\n",
      "1200\n",
      "1500\n",
      "1800\n",
      "2100\n",
      "2400\n",
      "2700\n",
      "3000\n",
      "3300\n",
      "3600\n",
      "3900\n",
      "4200\n",
      "4500\n",
      "4800\n",
      "preprocessing done 4999\n",
      "0  Score: -1.0\n",
      "1  Score: -1.0\n",
      "2  Score: -2.0\n",
      "3  Score: -1.0\n",
      "4  Score: 0.0\n",
      "5  Score: 0.0\n",
      "6  Score: 0.0\n",
      "7  Score: -1.0\n",
      "8  Score: 0.0\n",
      "9  Score: 0.0\n",
      "10  Score: 1.0\n",
      "11  Score: 0.0\n",
      "12  Score: -1.0\n",
      "13  Score: 1.0\n",
      "14  Score: 0.0\n",
      "15  Score: -3.0\n",
      "16  Score: 1.0\n",
      "17  Score: 0.0\n",
      "18  Score: 1.0\n",
      "19  Score: -1.0\n",
      "20  Score: -2.0\n",
      "21  Score: 1.0\n",
      "22  Score: 0.0\n",
      "23  Score: -2.0\n",
      "24  Score: 1.0\n",
      "25  Score: 1.0\n",
      "26  Score: 0.0\n",
      "27  Score: 0.0\n",
      "28  Score: 0.0\n",
      "29  Score: -3.0\n",
      "30  Score: 0.0\n",
      "31  Score: 0.0\n",
      "32  Score: -1.0\n",
      "33  Score: 2.0\n",
      "34  Score: 0.0\n",
      "35  Score: 1.0\n",
      "36  Score: 0.0\n",
      "37  Score: 0.0\n",
      "38  Score: 0.0\n",
      "39  Score: -1.0\n",
      "40  Score: 1.0\n",
      "41  Score: 0.0\n",
      "42  Score: 0.0\n",
      "43  Score: 2.0\n",
      "44  Score: 2.0\n",
      "45  Score: 2.0\n",
      "46  Score: 3.0\n",
      "47  Score: 0.0\n",
      "48  Score: 0.0\n",
      "49  Score: 3.0\n",
      "50  Score: 1.0\n",
      "51  Score: 0.0\n",
      "52  Score: 0.0\n",
      "53  Score: -1.0\n",
      "54  Score: -1.0\n",
      "55  Score: 2.0\n",
      "56  Score: 0.0\n",
      "57  Score: 1.0\n",
      "58  Score: 0.0\n",
      "59  Score: 0.0\n",
      "60  Score: -1.0\n",
      "61  Score: -3.0\n",
      "62  Score: -2.0\n",
      "63  Score: 0.0\n",
      "64  Score: 0.0\n",
      "65  Score: -1.0\n",
      "66  Score: 1.0\n",
      "67  Score: 0.0\n",
      "68  Score: 1.0\n",
      "69  Score: 0.0\n",
      "70  Score: 0.0\n",
      "71  Score: -1.0\n",
      "72  Score: 0.0\n",
      "73  Score: 0.0\n",
      "74  Score: 0.0\n",
      "75  Score: -2.0\n",
      "76  Score: 1.0\n",
      "77  Score: 0.0\n",
      "78  Score: 0.0\n",
      "79  Score: 0.0\n",
      "80  Score: 1.0\n",
      "81  Score: 1.0\n",
      "82  Score: 0.0\n",
      "83  Score: 0.0\n",
      "84  Score: 1.0\n",
      "85  Score: 1.0\n",
      "86  Score: 0.0\n",
      "87  Score: 0.0\n",
      "88  Score: 0.0\n",
      "89  Score: 0.0\n",
      "90  Score: -2.0\n",
      "91  Score: 1.0\n",
      "92  Score: 0.0\n",
      "93  Score: -2.0\n",
      "94  Score: -1.0\n",
      "95  Score: 1.0\n",
      "96  Score: -2.0\n",
      "97  Score: 1.0\n",
      "98  Score: 2.0\n",
      "99  Score: 2.0\n",
      "100  Score: 0.0\n",
      "101  Score: -1.0\n",
      "       101   suma ostatnich 100 : 3.0\n",
      "102  Score: 1.0\n",
      "       102   suma ostatnich 100 : 3.0\n",
      "103  Score: 0.0\n",
      "       103   suma ostatnich 100 : 6.0\n",
      "104  Score: 0.0\n",
      "       104   suma ostatnich 100 : 7.0\n",
      "105  Score: 1.0\n",
      "       105   suma ostatnich 100 : 7.0\n",
      "106  Score: 1.0\n",
      "       106   suma ostatnich 100 : 8.0\n",
      "107  Score: 0.0\n",
      "       107   suma ostatnich 100 : 9.0\n",
      "108  Score: -2.0\n",
      "       108   suma ostatnich 100 : 10.0\n",
      "109  Score: 0.0\n",
      "       109   suma ostatnich 100 : 8.0\n",
      "110  Score: 0.0\n",
      "       110   suma ostatnich 100 : 8.0\n",
      "111  Score: -2.0\n",
      "       111   suma ostatnich 100 : 7.0\n",
      "112  Score: -1.0\n",
      "       112   suma ostatnich 100 : 5.0\n",
      "113  Score: -1.0\n",
      "       113   suma ostatnich 100 : 5.0\n",
      "114  Score: 0.0\n",
      "       114   suma ostatnich 100 : 3.0\n",
      "115  Score: -1.0\n",
      "       115   suma ostatnich 100 : 3.0\n",
      "116  Score: 0.0\n",
      "       116   suma ostatnich 100 : 5.0\n",
      "117  Score: -1.0\n",
      "       117   suma ostatnich 100 : 4.0\n",
      "118  Score: 0.0\n",
      "       118   suma ostatnich 100 : 3.0\n",
      "119  Score: 0.0\n",
      "       119   suma ostatnich 100 : 2.0\n",
      "120  Score: 3.0\n",
      "       120   suma ostatnich 100 : 3.0\n",
      "121  Score: 0.0\n",
      "       121   suma ostatnich 100 : 8.0\n",
      "122  Score: 1.0\n",
      "       122   suma ostatnich 100 : 7.0\n",
      "123  Score: 1.0\n",
      "       123   suma ostatnich 100 : 8.0\n",
      "124  Score: 1.0\n",
      "       124   suma ostatnich 100 : 11.0\n",
      "125  Score: 0.0\n",
      "       125   suma ostatnich 100 : 11.0\n",
      "126  Score: -1.0\n",
      "       126   suma ostatnich 100 : 10.0\n",
      "127  Score: 0.0\n",
      "       127   suma ostatnich 100 : 9.0\n",
      "128  Score: 0.0\n",
      "       128   suma ostatnich 100 : 9.0\n",
      "129  Score: 0.0\n",
      "       129   suma ostatnich 100 : 9.0\n",
      "130  Score: -1.0\n",
      "       130   suma ostatnich 100 : 12.0\n",
      "131  Score: 1.0\n",
      "       131   suma ostatnich 100 : 11.0\n",
      "132  Score: 0.0\n",
      "       132   suma ostatnich 100 : 12.0\n",
      "133  Score: 1.0\n",
      "       133   suma ostatnich 100 : 13.0\n",
      "134  Score: 1.0\n",
      "       134   suma ostatnich 100 : 12.0\n",
      "135  Score: 1.0\n",
      "       135   suma ostatnich 100 : 13.0\n",
      "136  Score: 2.0\n",
      "       136   suma ostatnich 100 : 13.0\n",
      "137  Score: 0.0\n",
      "       137   suma ostatnich 100 : 15.0\n",
      "138  Score: 0.0\n",
      "       138   suma ostatnich 100 : 15.0\n",
      "139  Score: 0.0\n",
      "       139   suma ostatnich 100 : 15.0\n",
      "140  Score: 1.0\n",
      "       140   suma ostatnich 100 : 16.0\n",
      "141  Score: 3.0\n",
      "       141   suma ostatnich 100 : 16.0\n",
      "142  Score: 0.0\n",
      "       142   suma ostatnich 100 : 19.0\n",
      "143  Score: 0.0\n",
      "       143   suma ostatnich 100 : 19.0\n",
      "144  Score: 0.0\n",
      "       144   suma ostatnich 100 : 17.0\n",
      "145  Score: -1.0\n",
      "       145   suma ostatnich 100 : 15.0\n",
      "146  Score: 1.0\n",
      "       146   suma ostatnich 100 : 12.0\n",
      "147  Score: -1.0\n",
      "       147   suma ostatnich 100 : 10.0\n",
      "148  Score: -2.0\n",
      "       148   suma ostatnich 100 : 9.0\n",
      "149  Score: 0.0\n",
      "       149   suma ostatnich 100 : 7.0\n",
      "150  Score: 0.0\n",
      "       150   suma ostatnich 100 : 4.0\n",
      "151  Score: -1.0\n",
      "       151   suma ostatnich 100 : 3.0\n",
      "152  Score: 2.0\n",
      "       152   suma ostatnich 100 : 2.0\n",
      "153  Score: 0.0\n",
      "       153   suma ostatnich 100 : 4.0\n",
      "154  Score: 4.0\n",
      "       154   suma ostatnich 100 : 5.0\n",
      "155  Score: 1.0\n",
      "       155   suma ostatnich 100 : 10.0\n",
      "156  Score: -1.0\n",
      "       156   suma ostatnich 100 : 9.0\n",
      "157  Score: 2.0\n",
      "       157   suma ostatnich 100 : 8.0\n",
      "158  Score: 0.0\n",
      "       158   suma ostatnich 100 : 9.0\n",
      "159  Score: 0.0\n",
      "       159   suma ostatnich 100 : 9.0\n",
      "160  Score: 0.0\n",
      "       160   suma ostatnich 100 : 9.0\n",
      "161  Score: 0.0\n",
      "       161   suma ostatnich 100 : 10.0\n",
      "162  Score: 3.0\n",
      "       162   suma ostatnich 100 : 13.0\n",
      "163  Score: 2.0\n",
      "       163   suma ostatnich 100 : 18.0\n",
      "164  Score: 1.0\n",
      "       164   suma ostatnich 100 : 20.0\n",
      "165  Score: 0.0\n",
      "       165   suma ostatnich 100 : 21.0\n",
      "166  Score: 1.0\n",
      "       166   suma ostatnich 100 : 22.0\n",
      "167  Score: 0.0\n",
      "       167   suma ostatnich 100 : 22.0\n",
      "168  Score: -2.0\n",
      "       168   suma ostatnich 100 : 22.0\n",
      "169  Score: 0.0\n",
      "       169   suma ostatnich 100 : 19.0\n",
      "170  Score: 0.0\n",
      "       170   suma ostatnich 100 : 19.0\n",
      "171  Score: -3.0\n",
      "       171   suma ostatnich 100 : 19.0\n",
      "172  Score: 0.0\n",
      "       172   suma ostatnich 100 : 17.0\n",
      "173  Score: 0.0\n",
      "       173   suma ostatnich 100 : 17.0\n",
      "174  Score: -1.0\n",
      "       174   suma ostatnich 100 : 17.0\n",
      "175  Score: 1.0\n",
      "       175   suma ostatnich 100 : 16.0\n",
      "176  Score: 3.0\n",
      "       176   suma ostatnich 100 : 19.0\n",
      "177  Score: 2.0\n",
      "       177   suma ostatnich 100 : 21.0\n",
      "178  Score: 1.0\n",
      "       178   suma ostatnich 100 : 23.0\n",
      "179  Score: 0.0\n",
      "       179   suma ostatnich 100 : 24.0\n",
      "180  Score: -3.0\n",
      "       180   suma ostatnich 100 : 24.0\n",
      "181  Score: 0.0\n",
      "       181   suma ostatnich 100 : 20.0\n",
      "182  Score: 0.0\n",
      "       182   suma ostatnich 100 : 19.0\n",
      "183  Score: 2.0\n",
      "       183   suma ostatnich 100 : 19.0\n",
      "184  Score: -1.0\n",
      "       184   suma ostatnich 100 : 21.0\n",
      "185  Score: 1.0\n",
      "       185   suma ostatnich 100 : 19.0\n",
      "186  Score: -1.0\n",
      "       186   suma ostatnich 100 : 19.0\n",
      "187  Score: 2.0\n",
      "       187   suma ostatnich 100 : 18.0\n",
      "188  Score: 0.0\n",
      "       188   suma ostatnich 100 : 20.0\n",
      "189  Score: -1.0\n",
      "       189   suma ostatnich 100 : 20.0\n",
      "190  Score: 0.0\n",
      "       190   suma ostatnich 100 : 19.0\n",
      "191  Score: 0.0\n",
      "       191   suma ostatnich 100 : 21.0\n",
      "192  Score: -1.0\n",
      "       192   suma ostatnich 100 : 20.0\n",
      "193  Score: 0.0\n",
      "       193   suma ostatnich 100 : 19.0\n",
      "194  Score: 5.0\n",
      "       194   suma ostatnich 100 : 21.0\n",
      "195  Score: 0.0\n",
      "       195   suma ostatnich 100 : 27.0\n",
      "196  Score: 2.0\n",
      "       196   suma ostatnich 100 : 26.0\n",
      "197  Score: 2.0\n",
      "       197   suma ostatnich 100 : 30.0\n",
      "198  Score: 1.0\n",
      "       198   suma ostatnich 100 : 31.0\n"
     ]
    }
   ],
   "source": [
    "episode_score_buffer = []\n",
    "eps = eps_start\n",
    "preprocess()\n",
    "for episode in range(episode_number):\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    score = 0                                          # initialize the score\n",
    "    while True:\n",
    "        action = agent.choose_action(state, eps).astype(int)            # select an action\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        score += reward                                # update the score\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state                             # roll over the state to next time step\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    print(episode, \" Score: {}\".format(score))\n",
    "    if len(episode_score_buffer) > 100:\n",
    "        print(\"      \", episode, \"  sum of last 100 :\", np.sum(episode_score_buffer[-100:]))\n",
    "    eps = eps*eps_decay\n",
    "    if eps < eps_end:\n",
    "        eps = eps_end\n",
    "    episode_score_buffer.append(score)\n",
    "    if (len(episode_score_buffer) > 100) and (np.sum(episode_score_buffer[-100:])/100 >= 13):\n",
    "        print(\"End in episode \", episode)\n",
    "        torch.save(agent.localNetwork.state_dict(), 'checkpoint.pth')\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.localNetwork.load_state_dict(torch.load('checkpoint200.pth'))\n",
    "score =0\n",
    "while True:\n",
    "    action = agent.choose_action(state, 0.0).astype(int)            # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    print(score)\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(episode_score_buffer)), episode_score_buffer)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "state = env_info.vector_observations[0]\n",
    "score = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
